{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv('fhv_tripdata_2019-10.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = df.repartition(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df_part' is your repartitioned DataFrame\n",
    "parquet_path = \"file://\" + os.getcwd() + \"/your_file_name.parquet\"\n",
    "\n",
    "# Save DataFrame to Parquet file\n",
    "df_part.write.parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Filter records for the 15th of October\n",
    "records_on_15th_oct = df.filter(F.col('pickup_datetime').cast('date') == '2019-10-15')\n",
    "\n",
    "# Count the number of records\n",
    "count_records_on_15th_oct = records_on_15th_oct.count()\n",
    "\n",
    "print(f\"Number of records on 15th October: {count_records_on_15th_oct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"pickup_datetime\", F.to_timestamp(\"pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df = df.withColumn(\"dropOff_datetime\", F.to_timestamp(\"dropOff_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Calculate trip duration in seconds\n",
    "df = df.withColumn(\"trip_duration_seconds\", (F.col(\"dropOff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\")))\n",
    "\n",
    "# Group by date and find the maximum trip duration for each day\n",
    "max_trip_duration_per_day = df.groupBy(F.date_format(\"pickup_datetime\", \"yyyy-MM-dd\").alias(\"trip_date\")).agg(\n",
    "    F.max(\"trip_duration_seconds\").alias(\"max_trip_duration_seconds\")\n",
    ")\n",
    "\n",
    "# Convert seconds to hours\n",
    "max_trip_duration_per_day = max_trip_duration_per_day.withColumn(\n",
    "    \"max_trip_duration_hours\", F.col(\"max_trip_duration_seconds\") / 3600.0\n",
    ")\n",
    "\n",
    "# Show or print the information about the longest trip for each day\n",
    "max_trip_duration_per_day.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = spark.read.option(\"header\", \"true\").csv('taxi_zone_lookup.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df.join(zone, df.PUlocationID == zone.LocationID, \"left_outer\")\n",
    "\n",
    "# Group by zone name and count occurrences\n",
    "zone_counts = joined_df.groupBy(\"Zone\").agg(F.count(\"*\").alias(\"pickup_count\"))\n",
    "\n",
    "# Find the zone with the minimum count (least frequent)\n",
    "least_frequent_zone = zone_counts.orderBy(\"pickup_count\").first()\n",
    "\n",
    "# Extract the name of the least frequent pickup location zone\n",
    "least_frequent_zone_name = least_frequent_zone[\"Zone\"]\n",
    "\n",
    "print(f\"The name of the least frequent pickup location zone is: {least_frequent_zone_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
